{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_anytrading\n",
    "\n",
    "from gym_anytrading.envs import TradingEnv, ForexEnv, StocksEnv, Actions, Positions \n",
    "from gym_anytrading.datasets import FOREX_EURUSD_1H_ASK, STOCKS_GOOGL\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import applications\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, Input, Concatenate, Conv2D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.agents.ddpg import DDPGAgent\n",
    "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.random import OrnsteinUhlenbeckProcess\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint, TrainIntervalLogger\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        assert observation.ndim == 2  # (height, width, channel)\n",
    "        return self.process_obs_1(observation)\n",
    "    \n",
    "    def process_obs_1(self, observation):\n",
    "        prices = []\n",
    "        diff = []\n",
    "        for o in observation:\n",
    "            prices.append(o[0])\n",
    "            diff.append(o[1])\n",
    "        prices = preprocessing.normalize([prices], norm='l2', axis=1, copy=True, return_norm=False)[0]\n",
    "        diff = preprocessing.normalize([diff], norm='l2', axis=1, copy=True, return_norm=False)[0]\n",
    "        new_obs = np.column_stack((prices, diff))\n",
    "        return new_obs\n",
    "    \n",
    "    def process_obs_2(self, observation):\n",
    "        prices = []\n",
    "        diff = []\n",
    "        for o in observation:\n",
    "            prices.append(o[0])\n",
    "            diff.append(o[1] / o[0] * 100)\n",
    "        prices = preprocessing.normalize([prices], norm='l2', axis=1, copy=True, return_norm=False)[0]\n",
    "        new_obs = np.column_stack((prices, diff))\n",
    "        return new_obs\n",
    "        \n",
    "    def process_state_batch(self, batch):\n",
    "#         print(batch)\n",
    "        return batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return reward\n",
    "\n",
    "class DDPGEnvProcessor(EnvProcessor):\n",
    "    def process_action(self,action):\n",
    "        return np.argmax(action)\n",
    "    \n",
    "class CustomEpsGreedyQPolicy(EpsGreedyQPolicy):\n",
    "    def __init__(self, eps=0.1, update_interval=100):\n",
    "        EpsGreedyQPolicy.__init__(self, eps)\n",
    "        self.update_interval = update_interval\n",
    "        self.count = 0\n",
    "        self.init_eps = self.eps\n",
    "        \n",
    "    def select_action(self, q_values):\n",
    "        assert q_values.ndim == 1\n",
    "        nb_actions = q_values.shape[0]\n",
    "\n",
    "        if np.random.uniform() < self.eps:\n",
    "            action = np.random.randint(0, nb_actions)\n",
    "        else:\n",
    "            action = np.argmax(q_values)\n",
    "        self.count += 1\n",
    "        if (self.count % self.update_interval) == 0:\n",
    "            self.eps = self.init_eps / (self.count / self.update_interval)\n",
    "            print(self.eps)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 1, 10, 32)         544       \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 1, 10, 64)         16448     \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 1, 10, 128)        65664     \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 1, 10, 128)        131200    \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1024)              1311744   \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 3,626,850\n",
      "Trainable params: 3,626,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('forex-v0', frame_bound=(50, 4000), window_size=10)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Next, we build a very simple model.\n",
    "model = Sequential()\n",
    "model.add(Conv2D(input_shape=(1,) + env.observation_space.shape, filters=32, kernel_size=(4,2), padding='same'))\n",
    "model.add(Conv2D(filters=64, kernel_size=(4,2), padding='same'))\n",
    "model.add(Conv2D(filters=128, kernel_size=(4,2), padding='same'))\n",
    "model.add(Conv2D(filters=128, kernel_size=(4,2), padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(nb_actions, activation='linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 = enable_double_dqn=True, nb_steps=1000000, EpsGreedyQPolicy **\n",
    "# 4 = enable_double_dqn=True, nb_steps=100000, EpsGreedyQPolicy\n",
    "# 5 = enable_double_dqn=True, nb_steps=100000, BoltzmannQPolicy **\n",
    "# 6 = enable_double_dqn=True, nb_steps=1000000, CustomEpsGreedyQPolicy, process_obs_2\n",
    "# 7 = enable_double_dqn=True, nb_steps=1000000, EpsGreedyQPolicy, process_obs_2 **\n",
    "# 8 = enable_double_dqn=True, nb_steps=1000000, CustomEpsGreedyQPolicy **\n",
    "# 9 = enable_double_dqn=True, nb_steps=1000000, CustomEpsGreedyQPolicy(update_interval=2500), train_interval=256, batch_size=512\n",
    "# 10 = enable_double_dqn=True, nb_steps=1000000, BoltzmannQPolicy, train_interval=256, batch_size=512 **\n",
    "# 11 = DDPG, train_interval=256, batch_size=512 ***\n",
    "# 12 = enable_double_dqn=True, nb_steps=1000000, BoltzmannQPolicy, train_interval=32, batch_size=64 \n",
    "# 13 = DDPG, train_interval=128, batch_size=64 ***\n",
    "# 14 = DDPG, train_interval=128, batch_size=256, lr = 0.01, target_model_update=0.01, gamma=0.9\n",
    "# 15 = DDPG, train_interval=128, batch_size=256, lr = 0.0001, target_model_update=0.0001, gamma=0.9\n",
    "train_no = 14\n",
    "weights_filename = 'dqn_weights_{}.h5f'.format(train_no)\n",
    "checkpoint_weights_filename = 'dqn_weights_{step}_'+'{}.h5f'.format(train_no)\n",
    "log_filename = 'dqn_log_{}.csv'.format(train_no)\n",
    "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=680)]\n",
    "callbacks += [TrainIntervalLogger(interval=100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = EnvProcessor()\n",
    "memory = SequentialMemory(limit=100000, window_length=1)\n",
    "# policy = CustomEpsGreedyQPolicy(update_interval = 2500, eps = 1.0)\n",
    "policy = BoltzmannQPolicy()\n",
    "agent = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=20,\n",
    "               target_model_update=1e-2, policy=policy, batch_size=64, processor=processor, \n",
    "               train_interval=32, enable_double_dqn=True)\n",
    "agent.compile(Adam(lr=1e-3), metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 4000 steps ...\n",
      "Training for 4000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "100/100 [==============================] - 3s 26ms/step - reward: 2.4530\n",
      "Interval 2 (100 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: -0.2130\n",
      "Interval 3 (200 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: -1.7820\n",
      "Interval 4 (300 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: -1.8420\n",
      "Interval 5 (400 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: -0.2200\n",
      "Interval 6 (500 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: 0.3700\n",
      "Interval 7 (600 steps performed)\n",
      "100/100 [==============================] - 1s 12ms/step - reward: -0.0680\n",
      "Interval 8 (700 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: -0.2810\n",
      "Interval 9 (800 steps performed)\n",
      "100/100 [==============================] - 1s 7ms/step - reward: 0.5660\n",
      "Interval 10 (900 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: 0.6040\n",
      "Interval 11 (1000 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: 0.8770\n",
      "Interval 12 (1100 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: -8.8924e-14\n",
      "Interval 13 (1200 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: -1.9820\n",
      "Interval 14 (1300 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: -0.0970\n",
      "Interval 15 (1400 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: -0.7160\n",
      "Interval 16 (1500 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: 0.3080\n",
      "Interval 17 (1600 steps performed)\n",
      "100/100 [==============================] - 1s 7ms/step - reward: -0.0100\n",
      "Interval 18 (1700 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: 0.4350\n",
      "Interval 19 (1800 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: 2.8410\n",
      "Interval 20 (1900 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: -0.4230: 0s - reward: 0.4\n",
      "Interval 21 (2000 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: -0.2450\n",
      "Interval 22 (2100 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: 1.1640\n",
      "Interval 23 (2200 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: -1.0950\n",
      "Interval 24 (2300 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: -0.3750\n",
      "Interval 25 (2400 steps performed)\n",
      "100/100 [==============================] - 1s 7ms/step - reward: -0.1410\n",
      "Interval 26 (2500 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: 1.8520\n",
      "Interval 27 (2600 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: -1.2830\n",
      "Interval 28 (2700 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: 1.1830TA: 0s - reward:\n",
      "Interval 29 (2800 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: -0.5450\n",
      "Interval 30 (2900 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: -0.8490\n",
      "Interval 31 (3000 steps performed)\n",
      "100/100 [==============================] - 1s 7ms/step - reward: 0.1520\n",
      "Interval 32 (3100 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: -0.1350\n",
      "Interval 33 (3200 steps performed)\n",
      "100/100 [==============================] - 1s 7ms/step - reward: 0.8770\n",
      "Interval 34 (3300 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: -0.9510\n",
      "Interval 35 (3400 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: 0.2900\n",
      "Interval 36 (3500 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: -0.4430\n",
      "Interval 37 (3600 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: -0.3430\n",
      "Interval 38 (3700 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: 1.8400\n",
      "Interval 39 (3800 steps performed)\n",
      "100/100 [==============================] - 1s 6ms/step - reward: -0.4670\n",
      "Interval 40 (3900 steps performed)\n",
      " 37/100 [==========>...................] - ETA: 0s - reward: -0.7730  3949/4000: episode: 1, duration: 27.106s, episode steps: 3949, steps per second: 146, episode reward: 3.100, mean reward: 0.001 [-153.500, 195.200], mean action: 0.348 [0.000, 1.000], mean observation: 0.160 [-0.985, 0.991], loss: 52.572065, mean_absolute_error: 2.909108, mean_q: 0.726975\n",
      "100/100 [==============================] - 1s 6ms/step - reward: -1.6470\n",
      "done, took 27.362 seconds\n",
      "done, took 27.362 seconds\n"
     ]
    }
   ],
   "source": [
    "agent.fit(env, nb_steps=4000, visualize=False, verbose=2, callbacks=callbacks)\n",
    "\n",
    "# After training is done, we save the final weights.\n",
    "agent.save_weights('dqn_{}_weights.h5f'.format('forex-v0'), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward: -37.199999999999456, total profit: 0.9890048874492001\n"
     ]
    }
   ],
   "source": [
    "print(\"total reward: {}, total profit: {}\".format(env._total_reward, env._total_profit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1,) + (10,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
